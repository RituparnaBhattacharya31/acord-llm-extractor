# POC on ACORD Form Extraction Using AWS Lambda

![Python](https://img.shields.io/badge/python-3.12-blue)
![AWS Lambda](https://img.shields.io/badge/aws-lambda-orange)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Serverless](https://img.shields.io/badge/serverless-event--driven-lightgrey)

Event-driven ACORD 140 PDF extraction pipeline using AWS Lambda, Google Gemini, S3, and DynamoDB. Automatically converts ACORD forms into clean, structured JSON using serverless architecture and LLM intelligence.
Full Description
A fully serverless and automated pipeline for extracting structured JSON data from ACORD 140 insurance PDF forms using:
‚û°Ô∏è AWS Lambda (Python)
‚û°Ô∏è Google Gemini (LLM)
‚û°Ô∏è Amazon S3 event triggers
‚û°Ô∏è Lambda Layers (for 70MB Gemini dependencies)
‚û°Ô∏è DynamoDB storage

üöÄ Overview

This repository contains a complete Proof of Concept (POC) for extracting structured JSON data from ACORD 140 insurance PDF forms using:

‚û°Ô∏è AWS Lambda (Python)
‚û°Ô∏è Gemini LLM for field-level extraction
‚û°Ô∏è Amazon S3 for file ingestion & output
‚û°Ô∏è DynamoDB for storing extraction + validation results
‚û°Ô∏è Lambda Layers for large Python dependencies

The pipeline is fully automated: once a PDF arrives in S3, the Lambda extracts, validates, stores, and logs everything end-to-end.

This README provides an end-to-end guide on implementing an **ACORD 140
PDF ‚Üí JSON extraction workflow** using **AWS Lambda, S3, DynamoDB, and
Gemini LLM**.\
It consolidates all steps from local development, packaging, deployment,
and final testing.

------------------------------------------------------------------------

üèóÔ∏è Architecture Diagram

                    +-------------------+
                    |   Email / User    |
                    +---------+---------+
                              |
                              v
                    +-------------------+
                    |   ACORD PDFs in   |
                    |  S3 Input Bucket  |
                    +---------+---------+
                              |
                     S3 Event Trigger
                              |
                              v
    +-------------------------------------------------------+
    |                     AWS Lambda                        |
    |                                                       |
    | 1. Reads PDF from S3                                  |
    | 2. Extracts text using PDFProcessor                   |
    | 3. Sends extracted text to Gemini (LLMExtractor)      |
    | 4. Validates response (DataValidator)                 |
    | 5. Writes JSON to output S3 bucket                    |
    | 6. Stores record in DynamoDB                          |
    +-------------------------------------------------------+
                              |
                              v
                   +----------------------+
                   |      S3 Bucket       |
                   +----------------------+
                              |
                              v
                   +----------------------+
                   |   DynamoDB Table     |
                   +----------------------+

------------------------------------------------------------------------

# 1. Creating a Lambda in Python

AWS Lambda supports Python runtimes and is ideal for event-driven PDF
processing.

### Steps

1.  AWS Console ‚Üí Lambda ‚Üí Create Function\
2.  Select **Author from scratch**\
3.  Runtime: **Python 3.12**\
4.  Architecture: **x86_64**\
5.  Recommended settings:
    -   Timeout: **30--90 sec**\
    -   Memory: **1024--4096 MB**

------------------------------------------------------------------------

# 2. Testing Lambda Locally

Before deploying, test everything locally.

### Local Project Structure

    acord_lambda_container/
    ‚îÇ
    ‚îú‚îÄ‚îÄ clients/
    ‚îÇ   ‚îú‚îÄ‚îÄ base_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ extractors/
    ‚îÇ   ‚îú‚îÄ‚îÄ llm_extractor.py
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îú‚îÄ‚îÄ accord_schema.py
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ output/              # local output during local testing (ignored in Lambda)
    ‚îÇ   ‚îî‚îÄ‚îÄ *.json
    ‚îÇ
    ‚îú‚îÄ‚îÄ results/             # logs, samples for debugging (optional)
    ‚îÇ   ‚îî‚îÄ‚îÄ *.json
    ‚îÇ
    ‚îú‚îÄ‚îÄ tools/
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py          # entry point for local testing (NOT used in Lambda)
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ utils/
    ‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ validators/
    ‚îÇ   ‚îú‚îÄ‚îÄ data_validator.py
    ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ venv/                # local virtual environment (ignored in Git + Lambda)
    ‚îÇ
    ‚îú‚îÄ‚îÄ .env                 # local environment variables (never commit to Git)
    ‚îú‚îÄ‚îÄ 140-Property-Acord.pdf
    ‚îú‚îÄ‚îÄ build_push.sh        # optional helper script
    ‚îú‚îÄ‚îÄ config.py
    ‚îú‚îÄ‚îÄ Dockerfile           # used to build Linux-compatible wheels for Lambda Layer
    ‚îú‚îÄ‚îÄ handler.py           # AWS Lambda handler
    ‚îú‚îÄ‚îÄ README.md
    ‚îî‚îÄ‚îÄ requirements.txt

### Steps to run locally

    python -m venv venv
    .\venv\Scripts\Activate.ps1
    pip install -r requirements.txt

Create a `.env` file:

    GEMINI_API_KEY=AIzaxxxxxxxxxxxxxxx
    DDB_TABLE=ACORD140Records
    OUTPUT_BUCKET=acord-json-output
    LOG_LEVEL=INFO
    MODEL_NAME=gemini-3-pro-preview

Run the project:

    python -m main.py

------------------------------------------------------------------------

# 3. Creating S3 Bucket for Incoming ACORD Forms

This bucket stores ACORD PDFs coming from emails or UIs.

### Steps

1.  S3 ‚Üí Create Bucket\
2.  Example structure:

```{=html}
<!-- -->
```
    attachments/
       ACORD140-form.pdf

------------------------------------------------------------------------

# 4. Creating S3 Bucket for Output JSON

The Lambda writes parsed JSON to this bucket.

Example:

    acord-json-output/
       results/
          acord140-output.json

------------------------------------------------------------------------

# 5. Creating DynamoDB Table

Stores extracted JSON + metadata + validation output.

### Table Name

`ACORD140Records`

### Key Schema

  Column       Type     Notes
  ------------ -------- ---------------
  documentId   String   Partition Key
  createdAt    String   Sort Key

### Additional Attributes

-   s3Key
-   s3Bucket
-   extracted
-   validation

### Example Item

``` json
{
  "documentId": "POL123",
  "s3Key": "attachments/ACORD140.pdf",
  "s3Bucket": "acord-input",
  "extracted": {},
  "validation": {},
  "createdAt": "2025-12-06T20:45:31Z"
}
```

------------------------------------------------------------------------

# 6. Deploying AWS Lambda (Tricky Part)

Gemini Python libraries are **\~70MB** so they cannot be uploaded
directly.\
We use **Lambda Layers** built in **Linux**.

You can do this in two ways:

Option A ‚Äî Using Docker (Recommended)
    Build dependencies inside a container that matches the Lambda runtime:
    docker build -t acord-layer-builder .
    docker run --rm -v ${PWD}:/output acord-layer-builder
    cd python
    zip -r ../python_libs.zip .


Option B ‚Äî Using Ubuntu (WSL 2)
    Install dependencies inside Ubuntu to match Lambda‚Äôs Linux OS:
    mkdir python
    pip install -r requirements.txt -t python/
    zip -r python_libs.zip python/

------------------------------------------------------------------------

## 6.1 Generating Linux-Compatible Packages

### Steps

1.  Install Ubuntu (WSL2)\
2.  Inside Ubuntu:

```{=html}
<!-- -->
```
    mkdir python
    pip install -r requirements.txt -t python/

Folder output:

    python/
       google/
       pydantic/
       ...

Create zip:

    zip -r python_libs.zip python/

Size \~71 MB.

------------------------------------------------------------------------

## 6.2 Uploading Layer ZIP to S3

Move `python_libs.zip` to Windows ‚Üí Upload to S3.

------------------------------------------------------------------------

## 6.3 Creating Lambda Layer

1.  Lambda ‚Üí Layers ‚Üí Create Layer\
2.  Choose **Upload from S3**\
3.  Runtime: **Python 3.12**\
4.  Architecture: **x86_64**

------------------------------------------------------------------------

## 6.4 Creating the Lambda Function

Zip the Lambda code:

    Compress-Archive -Path .\utils\, .\clients\, .\extractors\, .\models\, .\tools\, .\validators\, .\handler.py, .\config.py -DestinationPath root.zip

Upload to Lambda.

------------------------------------------------------------------------

## 6.5 Environment Variables

    GEMINI_API_KEY=AIzaxxxxxxxxxxxxxxxxxxxxx
    DDB_TABLE=ACORD140Records
    OUTPUT_BUCKET=acord-json-output
    LOG_LEVEL=INFO
    MODEL_NAME=gemini-3-pro-preview

------------------------------------------------------------------------

# 7. Adding Lambda Layer

Lambda ‚Üí Layers ‚Üí Add Layer ‚Üí Custom Layer ‚Üí Select version.

------------------------------------------------------------------------

# 8. Adding S3 Trigger

Steps: 1. Lambda ‚Üí Triggers ‚Üí Add Trigger\
2. Select S3 bucket\
3. Event: **ObjectCreated:Put**\
4. Prefix: `attachments/`

------------------------------------------------------------------------

# 9. IAM Permissions for Lambda

### S3 Read (input bucket)

``` json
{
  "Effect": "Allow",
  "Action": ["s3:GetObject"],
  "Resource": "arn:aws:s3:::input-bucket-name/*"
}
```

### S3 Write (output bucket)

``` json
{
  "Effect": "Allow",
  "Action": ["s3:PutObject"],
  "Resource": "arn:aws:s3:::output-bucket-name/*"
}
```

### DynamoDB Permissions

``` json
{
  "Effect": "Allow",
  "Action": ["dynamodb:PutItem", "dynamodb:UpdateItem"],
  "Resource": "arn:aws:dynamodb:us-east-1:123456789012:table/dynamo-db-name"
}
```

------------------------------------------------------------------------

# 10. Testing End-to-End

### Steps

1.  Upload ACORD140 PDF to the input bucket
2.  Lambda gets triggered
3.  Output JSON appears in the output bucket
4.  DynamoDB item is created
5.  CloudWatch logs show execution details

------------------------------------------------------------------------

# ‚úîÔ∏è Summary

This pipeline enables: - Automated ACORD PDF ‚Üí JSON extraction\
- LLM-powered parsing
- Serverless processing
- End-to-end cloud automation
